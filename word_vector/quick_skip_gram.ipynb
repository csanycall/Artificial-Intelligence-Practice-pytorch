{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0,allloss=166.3806,loss=11.0920\n",
      "epoch=0,allloss=402754.0525,loss=9.6811\n",
      "epoch=0,allloss=765802.5440,loss=9.1182\n",
      "epoch=0,allloss=1114675.9577,loss=8.8611\n",
      "epoch=0,allloss=1463813.5307,loss=8.6827\n",
      "epoch=0,allloss=1807101.8429,loss=8.5718\n",
      "epoch=0,allloss=2144485.2823,loss=8.4792\n",
      "epoch=0,allloss=2480586.0877,loss=8.4105\n",
      "epoch=0,allloss=2816309.2116,loss=8.3553\n",
      "epoch=0,allloss=3158292.2935,loss=8.3367\n",
      "epoch=0,allloss=3500108.7700,loss=8.3041\n",
      "epoch=0,allloss=3838008.4798,loss=8.2757\n",
      "epoch=0,allloss=4175995.7636,loss=8.2553\n",
      "epoch=0,allloss=4516629.6000,loss=8.2361\n",
      "epoch=0,allloss=4850940.0141,loss=8.2210\n",
      "epoch=0,allloss=5190263.7328,loss=8.2089\n",
      "epoch=0,allloss=5529253.1796,loss=8.2002\n",
      "epoch=0,allloss=5870579.7614,loss=8.1937\n",
      "epoch=0,allloss=6217268.5927,loss=8.1878\n",
      "epoch=0,allloss=6565514.4718,loss=8.1890\n",
      "epoch=0,allloss=6905893.7228,loss=8.1850\n",
      "epoch=0,allloss=7248909.3811,loss=8.1806\n",
      "epoch=0,allloss=7260258.0207,loss=8.1804\n",
      "epoch=0,allloss=121.2186,loss=8.6585\n",
      "epoch=0,allloss=343847.7110,loss=8.2048\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordEmbSkip(nn.Module):\n",
    "    def __init__(self,nwords,emb_size):\n",
    "        super(WordEmbSkip,self).__init__()\n",
    "        self.word_emb=nn.Embedding(nwords,emb_size,sparse=True)\n",
    "        nn.init.xavier_uniform_(self.word_emb.weight)\n",
    "        self.context_emb=nn.Embedding(nwords,emb_size,sparse=True)\n",
    "        nn.init.xavier_uniform_(self.context_emb.weight)\n",
    "    \n",
    "    def forward(self,words_pos,context_positions,negative_sample=False):\n",
    "        word_emb=self.word_emb(words_pos)\n",
    "        context_emb=self.context_emb(context_positions)\n",
    "        score=torch.matmul(word_emb,context_emb.transpose(dim0=1,dim1=0))\n",
    "        if negative_sample:#负样本就是父分数\n",
    "            score=-1*score\n",
    "        obj=-1*torch.sum(F.logsigmoid(score))\n",
    "        return obj\n",
    "\n",
    "K=3#负样本的样本数\n",
    "N=2#窗口大小\n",
    "EMB_SIZE=128\n",
    "embeddings_location=\"embeddings.txt\"\n",
    "labels_location=\"labels.txt\"\n",
    "w2i=defaultdict(lambda:len(w2i))\n",
    "word_counts=defaultdict(int)#记录每个词出现的次数\n",
    "S=w2i[\"<s>\"]\n",
    "UNK=w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip().split(\" \")\n",
    "            for word in line:\n",
    "                word_counts[w2i[word]]+= 1\n",
    "            yield[w2i[x] for x in line]\n",
    "train=list(read_dataset(\"Demo/DataSets/train.txt\"))\n",
    "w2i=defaultdict(lambda :UNK,w2i)\n",
    "dev=list(read_dataset(\"Demo/DataSets/valid.txt\"))\n",
    "i2w={v:k for k,v in w2i.items()}\n",
    "nwords=len(w2i)\n",
    "\n",
    "#归一化\n",
    "counts=np.array([list(x) for x in word_counts.items()])[:,1]**.75\n",
    "normalizing_constant=sum(counts)\n",
    "word_probabilities=np.zeros(nwords)\n",
    "for word_id in word_counts:\n",
    "    word_probabilities[word_id]=word_counts[word_id]**.75/normalizing_constant\n",
    "\n",
    "with open(labels_location,'w') as f:\n",
    "    for i in range(nwords):\n",
    "        f.write(i2w[i]+\"\\n\")\n",
    "\n",
    "model =WordEmbSkip(nwords,EMB_SIZE)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.1)\n",
    "def sent_loss(sent):\n",
    "    all_neg_words = np.random.choice(nwords, size=2*N*K*len(sent), replace=True, p=word_probabilities)\n",
    "    losses=[]\n",
    "    for i,word in enumerate(sent):\n",
    "        pos_words = [sent[x] if x >= 0 else S for x in range(i-N,i)] + [sent[x] if x < len(sent) else S for x in range(i+1,i+N+1)]\n",
    "        pos_words_tensor = torch.tensor(pos_words)\n",
    "        neg_words = all_neg_words[i*K*2*N:(i+1)*K*2*N]\n",
    "        neg_words_tensor = torch.tensor(neg_words)\n",
    "        target_word_tensor = torch.tensor([word])\n",
    "        pos_loss = model(target_word_tensor, pos_words_tensor)\n",
    "        neg_loss = model(target_word_tensor, neg_words_tensor, negative_sample=True)\n",
    "\n",
    "        losses.append(pos_loss + neg_loss)\n",
    "\n",
    "    return torch.stack(losses).sum()\n",
    "for epoch in range(1):\n",
    "    random.shuffle(train)\n",
    "    train_words,train_loss=0,0.0\n",
    "    model.train()\n",
    "    for sent_id,sent in enumerate(train):\n",
    "        my_loss=sent_loss(sent)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=my_loss.item()\n",
    "        train_words+=len(sent)\n",
    "        if(sent_id%2000==0):\n",
    "            print(\"epoch=%r,allloss=%.4f,loss=%.4f\"%(epoch,train_loss,train_loss/train_words))\n",
    "    print(\"epoch=%r,allloss=%.4f,loss=%.4f\"%(epoch,train_loss,train_loss/train_words))\n",
    "    model.eval()\n",
    "    dev_words,dev_loss=0,0.0\n",
    "    for sent_id,sent in enumerate(dev):\n",
    "        my_loss=sent_loss(sent)\n",
    "        dev_loss+=my_loss.item()\n",
    "        dev_words+=len(sent)\n",
    "        if(sent_id%2000==0):\n",
    "            print(\"epoch=%r,allloss=%.4f,loss=%.4f\"%(epoch,dev_loss,dev_loss/dev_words))\n",
    "    if(sent_id%2000==0):\n",
    "            print(\"epoch=%r,allloss=%.4f,loss=%.4f\"%(epoch,dev_loss,dev_loss/dev_words))\n",
    "            \n",
    "with open(embeddings_location,\"w\") as f:\n",
    "    W_np=model.word_emb.weight.data.numpy()\n",
    "    for i in range(nwords):\n",
    "        ith_embeddings='\\t'.join(map(str,W_np[i]))\n",
    "        f.write(ith_embeddings+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
