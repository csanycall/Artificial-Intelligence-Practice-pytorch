{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "uuid": "26d413fa-0f95-4a0c-8ec0-fb45bad1cedb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "002fe592-88fb-4489-bbf9-bb8a7b2a4447"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "uuid": "1aa3a4b1-f418-48a3-8a7a-ba157c7aba33"
   },
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    #f就表示整个文件内容\n",
    "    rst=[]#存储整个文件的诗\n",
    "    with open(filename,\"rb\") as f:\n",
    "        datas=json.load(f)\n",
    "        for data in datas:#datas是一个列表，每个元素是一个字典，包含一首诗\n",
    "            paragraphs=data.get(\"paragraphs\")#['行行西至一荒陂，因笑唐公不見機。', '莫惜驌驦輸令尹，漢東宮闕早時歸。']\n",
    "            all_data=\"\"#存储每一部诗\n",
    "            for words in paragraphs:#遍历诗的每一句\n",
    "                all_data+=words\n",
    "            if all_data !=\"\":\n",
    "                #all_data=sentenceParse(all_data)\n",
    "                rst.append(all_data)\n",
    "    return rst\n",
    "            #all_data=sentenceParse(all_data)\n",
    "            #print(all_data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "uuid": "3bc013d0-02a1-4ab9-aa7c-b8fc18e9117d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3504\n"
     ]
    }
   ],
   "source": [
    "datas=[]#所有的唐诗\n",
    "file_path=\"./chinese-poetry/\"\n",
    "#获取所有的唐诗文件\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.startswith(\"poet.tang\"):\n",
    "        rst=read_json(file_path+filename)\n",
    "        #print(len(rst))\n",
    "        datas.extend(rst)\n",
    "w2i={}\n",
    "\n",
    "for words in datas:\n",
    "    for word in words:\n",
    "        if word not in w2i:\n",
    "            w2i[word]=len(w2i)\n",
    "\n",
    "w2i['<EOP>'] = len(w2i)\n",
    "w2i['<START>'] = len(w2i)\n",
    "VOCAB_SIZE = len(w2i)\n",
    "print(VOCAB_SIZE)\n",
    "f=open(\"w2i.txt\",\"wb\")\n",
    "f.write(pickle.dumps(w2i))\n",
    "f.close()#关闭文件，刷新缓冲区的数据\n",
    "#给每一诗后面中添加上\"<EOP>\",而且现在的data[i]为一个列表，列表中每一个元素是一个字\n",
    "for i in range(len(datas)):\n",
    "    datas[i] = list(datas[i])\n",
    "    datas[i].append(\"<EOP>\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "uuid": "8477504b-8eb5-4afd-8ef0-921c14074047"
   },
   "outputs": [],
   "source": [
    "class Language_Module(nn.Module):\n",
    "    def __init__(self,nwords,emb_size,hidden_size):\n",
    "        super(Language_Module,self).__init__()\n",
    "        self.emb=nn.Embedding(nwords,emb_size)\n",
    "        self.rnn=nn.LSTM(emb_size,hidden_size)\n",
    "        self.out=nn.Linear(hidden_size,nwords)\n",
    "    def forward(self,input):\n",
    "        embed=self.emb(input)\n",
    "        embed=embed.unsqueeze(1)\n",
    "        rnn_out,_=self.rnn(embed)\n",
    "        h,b,w=rnn_out.shape\n",
    "        out=rnn_out.view(-1,w)\n",
    "        out=self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "uuid": "98bb5542-19a7-4ae8-8e53-bb5e5af66d45"
   },
   "outputs": [],
   "source": [
    "#取出字典中键所对应的值\n",
    "def make_one_hot_vec_target(word, w2i):\n",
    "    rst = torch.LongTensor([w2i[word]])\n",
    "    return rst\n",
    "def makeForOneCase(sentence, one_hot_var_target):\n",
    "    tmpIn = []\n",
    "    tmpOut = []\n",
    "    #print(s)\n",
    "    #遍历整首诗，从整首诗的第二个开始\n",
    "    for i in range(1, len(sentence)):\n",
    "        word_out = sentence[i]\n",
    "        word_in = sentence[i - 1]\n",
    "        tmpIn.append(one_hot_var_target[word_in])\n",
    "        tmpOut.append(one_hot_var_target[word_out])\n",
    "    return torch.cat(tmpIn), torch.cat(tmpOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "uuid": "8d677567-8902-45f3-8550-c0cb8fb85003"
   },
   "outputs": [],
   "source": [
    "model=Language_Module(len(w2i),256,128)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "one_hot_var_target = {}\n",
    "for w in w2i:\n",
    "    one_hot_var_target.setdefault(w, make_one_hot_vec_target(w, w2i))\n",
    "    #setdefault的意思是说如果字典中没有w则设置w的值为make_one_hot_vec_target(w, w2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "uuid": "c8a3a553-de42-4099-976b-d7adc8e945de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 8.179521560668945\n",
      "0 7.077409267425537\n"
     ]
    }
   ],
   "source": [
    "epochNum = 1#epoch的次数\n",
    "\n",
    "TRAINSIZE = len(datas)#唐诗有多少首\n",
    "batch = 200\n",
    "for epoch in range(epochNum):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_counts = 0\n",
    "    for case in range(TRAINSIZE):\n",
    "        s = datas[case]\n",
    "        t, o = makeForOneCase(s, one_hot_var_target)\n",
    "        #print(t)\n",
    "        #print(o)\n",
    "        output = model(t)\n",
    "        loss=criterion(output, o)\n",
    "        train_loss += loss\n",
    "        train_counts += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if case%3==0:\n",
    "            print (epoch, (train_loss.float()/train_counts).item())\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    dev_counts = 0\n",
    "    for case in range(TRAINSIZE):\n",
    "        s = datas[case]\n",
    "        t, o = makeForOneCase(s, one_hot_var_target)\n",
    "        output = model(t)\n",
    "        dev_loss += criterion(output, o)\n",
    "        dev_counts += 1\n",
    "    print (epoch,(dev_loss.float()/dev_counts).item())\n",
    "    \n",
    "torch.save(model,'poetry-gen.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "uuid": "e31b147b-b5c6-4a31-a01d-bbf22ee37593"
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model = torch.load('poetry-gen.pt')\n",
    "#生成古诗的最大长度为100\n",
    "max_length = 100\n",
    "#读取字典\n",
    "w2i = open('w2i.txt', 'rb')\n",
    "#加载字典\n",
    "word_to_ix = pickle.load(w2i)\n",
    "#生成i2w，就是将w2i反转\n",
    "def i2w(w2i):\n",
    "    return dict((v, k) for k, v in w2i.items())\n",
    "ix_to_word = i2w(word_to_ix)\n",
    "\n",
    "def sample(startWord='<START>'):\n",
    "    #将开始词数字化\n",
    "    input = make_one_hot_vec_target(startWord, word_to_ix)\n",
    "    #记录生成的每一个词，从<START>开始\n",
    "    output_name = \"\";\n",
    "    if (startWord != \"<START>\"):\n",
    "        output_name = startWord\n",
    "    for i in range(max_length):\n",
    "        output = model(input)#输入为一个字\n",
    "        #topk为查找最大的数字topk和对应索引topi\n",
    "        topv, topi = output.data.topk(1)\n",
    "        #对应的索引，也就是字典中对应的那个索引\n",
    "        topi = topi[0][0]\n",
    "        #将索引转换为word\n",
    "        w = ix_to_word[topi.item()]\n",
    "        #只要w不是结束符就不结束\n",
    "        if w == \"<EOP>\":\n",
    "            break\n",
    "        else:\n",
    "            output_name += w\n",
    "        input = make_one_hot_vec_target(w, word_to_ix)\n",
    "    return output_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "ffd8e043-1fc6-4f7b-9b36-7681c515203a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "01a07e23-2b45-4eb4-8a5f-2ebbcd0c1601"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
